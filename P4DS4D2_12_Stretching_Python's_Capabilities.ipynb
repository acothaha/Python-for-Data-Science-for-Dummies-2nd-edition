{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding classes in Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining applications for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Estimator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:(506, 13) y:(506,)\n"
     ]
    }
   ],
   "source": [
    "# import datasets that Scikit-learn provided\n",
    "from sklearn.datasets import load_boston\n",
    "# assign the datasets into a variable\n",
    "boston = load_boston()\n",
    "# assign both of boston predictors and target\n",
    "x, y = boston.data, boston.target\n",
    "# figuring out the size of x and y\n",
    "print(f\"X:{x.shape} y:{y.shape}\")\n",
    "# the output shows both arrays have the same number of\n",
    "# roms and x has 13 features/variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n"
     ]
    }
   ],
   "source": [
    "# import LinearRegression class from linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# assign LinearRegression class into hypothesis\n",
    "# with a normalization\n",
    "hypothesis = LinearRegression(normalize=True)\n",
    "# fitting hypothesis into the predictors and outcome\n",
    "hypothesis.fit(x, y)\n",
    "# printing the all 13 (number of features in x) \n",
    "# LinearRegression coef. \n",
    "print(hypothesis.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Predictor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    0.    1.    0.    0.5   7.   59.    6.    3.  200.   20.  350.\n",
      "    4. ]]\n",
      "[25.90156732]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# making an array that consists of 13 features\n",
    "# necessary to be predicted using hypothesis earlier\n",
    "new_observation = np.array([1, 0, 1, 0, 0.5, 7, 59,\n",
    "                             6, 3, 200, 20, 350, 4],\n",
    "                        # dtype should be float to ensure 0.5 is 0.5\n",
    "                        # reshape is necessary to ensure it counts as\n",
    "                        # a 1 dimensional array\n",
    "                            dtype=float).reshape(1, -1)\n",
    "print(new_observation)\n",
    "# predict the outcome from the new_observation\n",
    "print(hypothesis.predict(new_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of the fit (R^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406426641094095"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Transform class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01116872 0.         0.01979472 0.         0.23662551 0.65893849\n",
      "  0.57775489 0.44288845 0.08695652 0.02480916 0.78723404 0.88173887\n",
      "  0.06263797]]\n"
     ]
    }
   ],
   "source": [
    "# because LinearRegression doesn't provide a transformation\n",
    "# we import MinMaxScaler to be used as a transformator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# assign MinMaxScaler to variable scaler\n",
    "# feature_range is a para. to set the min and max value\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(x)\n",
    "# transform new_observation with scaler\n",
    "print(scaler.transform(new_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the Hasing Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using hash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the hashing trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing how built-in fuction python to hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8083329979669929672\n"
     ]
    }
   ],
   "source": [
    "print(hash(\"Python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### return and index in a specific positive range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(abs(hash(\"Python\")) % 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying one-hot encoding using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 4, 'for': 1, 'data': 0, 'science': 5, 'machine': 3, 'learning': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "# create a encoder to hold a list of vectorized words(?)\n",
    "oh_encoder = CountVectorizer()\n",
    "# fitting the encoder and transforming it in the same time\n",
    "oh_encoded = oh_encoder.fit_transform(['Python for data science', \n",
    "                                       'Python for machine learning'])\n",
    "# display the words and its code\n",
    "print(oh_encoder.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple hashing trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 2 string as the input\n",
    "string_1 = 'Python for data science'\n",
    "string_2 = 'Python for machine learning'\n",
    "\n",
    "# defining a function to work as a hashing trick\n",
    "# first para. is the input\n",
    "# second para. is the vector size, set to 20 if not specified\n",
    "def hashing_trick(input_string, vector_size=20):\n",
    "    # creating the vector\n",
    "    feature_vector = [0] * 20\n",
    "    # using for loop to chech every single word in the input\n",
    "    for word in input_string.split(' '):\n",
    "        # defining the index using built-in func. hash()\n",
    "        # and because it can be a negative num. so abs() is \n",
    "        # necessary\n",
    "        # % vector_size is functioned as the index para.\n",
    "        # so that the index will not be more than its value\n",
    "        index = abs(hash(word)) % vector_size\n",
    "        # change the number to 1 designated to its index\n",
    "        feature_vector[index] = 1\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing both string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(hashing_trick(string_1))\n",
    "print(hashing_trick(string_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with deterministic selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "# creating a vector using previous hashing_trick function\n",
    "x = hashing_trick(string_1)\n",
    "# using sparse matrix to just find the non-0 value\n",
    "# in the vector\n",
    "print(csc_matrix(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x20 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text as txt\n",
    "\n",
    "string_1 = 'Python for data science'\n",
    "string_2 = 'Python for machine learning'\n",
    "# creating the hashingvectorizer\n",
    "# n_features para. defines how long the sparse matrix will be\n",
    "# binary para. defines wether the matrix will consist of binary value\n",
    "# norm. para. defines whether the matrix is normalized or nah\n",
    "h_trick = txt.HashingVectorizer(n_features=20,\n",
    "                                binary=True, norm=None)\n",
    "# transforming the input with the HashingVectorizer\n",
    "hashed_text = h_trick.transform([string_1, string_2])\n",
    "hashed_text\n",
    "# print(hashed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oh_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7bead7e7346a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# using one-hot encoding from previous try\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# todense func. used to return matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moh_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstring_3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# the output is all zero vector because the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# one-hot encoding func. hasn't updated yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'oh_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# define a new string/input to be inputted\n",
    "string_3 = 'New text has arrived'\n",
    "# using one-hot encoding from previous try\n",
    "# todense func. used to return matrix\n",
    "oh_encoder.transform([string_3]).todense()\n",
    "# the output is all zero vector because the \n",
    "# one-hot encoding func. hasn't updated yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using HashingVectorizer will automaticall add\n",
    "# the new input to the matrix\n",
    "h_trick.transform([string_3]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering Timing and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking with timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing %timeit and %%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.6 ms ± 3.91 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# the uses of %timeit by assign a list 10^6 ordinal values\n",
    "# the %timeit will only count its row\n",
    "%timeit l = [k for k in range(10**6)]\n",
    "# you can't call the variable listed in a %timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.2 ms ± 1.56 ms per loop (mean ± std. dev. of 5 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 20 -r 5 l = [k for k in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 ms ± 3.81 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "# using %% timeit to count the time for the entire cell\n",
    "l = []\n",
    "for k in range(10**6):\n",
    "    l.append(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing %timeit on a different text encoding strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# testing on one-hot encoder/Countvectorizer\n",
    "%timeit oh_encoded = oh_encoder.fit_transform([string_1, string_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 µs ± 4.11 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# testing on HashingVectorizer\n",
    "%timeit hashing = h_trick.transform([string_1, string_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the memory profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing memory_profiler package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in c:\\users\\aco\\anaconda3\\lib\\site-packages (0.57.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aco\\anaconda3\\lib\\site-packages (from memory_profiler) (5.6.7)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "# use this magic func. in every session you want to monitor\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 86.10 MiB, increment: 0.05 MiB\n"
     ]
    }
   ],
   "source": [
    "hashing = h_trick.transform([string_1, string_2])\n",
    "# %memit used to track the memory consumption\n",
    "%memit dense_hashing = hashing.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining a complete overview of memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example_code.py\n"
     ]
    }
   ],
   "source": [
    "%%file example_code.py\n",
    "# creating a file named example_code.py\n",
    "# function of 2 alternative text encoding strategies\n",
    "def comparison_test(text):\n",
    "    import sklearn.feature_extraction.text as txt\n",
    "    h_trick = txt.HashingVectorizer(n_features=20, binary=True,\n",
    "                                    norm=None)\n",
    "    oh_encoder = txt.CountVectorizer()\n",
    "    oh_encoded = oh_encoder.fit_transform(text)\n",
    "    hashing = h_trick.transform(text)\n",
    "    return oh_encoded, hashing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import the function earlier\n",
    "from example_code import comparison_test\n",
    "text = ['Python for data science',\n",
    "        'Python for machine learning']\n",
    "#  show the overview of the testing of the function called earlier\n",
    "# in a new window\n",
    "%mprun -f comparison_test comparison_test(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running in Parallel on Multiple Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing multicore parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using multiprocessing by Support Vector Classifier (SVC) and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45 s ± 33.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "# assign the dataset into a variable\n",
    "digits = load_digits()\n",
    "# assign digits.data to x\n",
    "# assgin digits.target(the classification target) to y\n",
    "x, y = digits.data, digits.target\n",
    "# import SVC which is the algorithm u\n",
    "from sklearn.svm import SVC\n",
    "# cross_val_score used to evaluates score by cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# counting the time of cross-validation using SVC algorithm\n",
    "# 1st para. is the algortihm/object used to fit the data\n",
    "# 2nd para. is the data to fit\n",
    "# 3rd para. is the target variable to try to predict\n",
    "# cv para. is how many folds you determine for cross_validation strategy\n",
    "# n_jobs para. used to determine how many core used\n",
    "%timeit single_core = cross_val_score(SVC(), x, y, \\\n",
    "                                      cv=20, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multicore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 s ± 48.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# n_jobs=-1 means you use all of the cores available\n",
    "%timeit multi_core = cross_val_score(SVC(), x, y, \\\n",
    "                                     cv=20, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to avoiding error caused by internal operations of a multicore task, happens a lot in console or IDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "if __name__ == '__main__':\n",
    "    digits = load_digits()\n",
    "    x, y = digits.data, digits.target\n",
    "    multi_core = cross_val_score(SVC(), x, y,\n",
    "                                 cv=20, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
